{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c264416",
   "metadata": {},
   "source": [
    "Goal is to develop machine learning code that takes input photon data from an array of my hodoscope and outputs where the beam impinged on the scintillating crystal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f08fd6",
   "metadata": {},
   "source": [
    "Google Colab Installation of mlinphysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e70f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB_FOLDER = 'MLP' # change as needed\n",
    "GITHUB_USER  = 'hbprosper'\n",
    "GITHUB_REPO  = 'mlinphysics'\n",
    "GITHUB_FOLDERS = ['mlinphysics']\n",
    "#------------------------------------------------------\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    print('\\nGoogle Drive mounted\\n')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    print('\\nRunning locally\\n')\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    MYDRIVE     = '/content/gdrive/MyDrive'\n",
    "    GITHUB_BASE = 'https://raw.githubusercontent.com'\n",
    "    MAIN        = 'refs/heads/main'\n",
    "    GITHUB_PATH = f'{MYDRIVE}/{COLAB_FOLDER}'\n",
    "    #------------------------------------------------------\n",
    "    %cd {GITHUB_PATH}\n",
    "    %rm -f {GITHUB_PATH}/clone2colab.ipynb\n",
    "    !wget -q {GITHUB_BASE}/{GITHUB_USER}/{GITHUB_REPO}/{MAIN}/clone2colab.ipynb\n",
    "    %run {GITHUB_PATH}/clone2colab.ipynb\n",
    "\n",
    "    %pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb438b52",
   "metadata": {},
   "source": [
    "Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard system modules\n",
    "import os, sys\n",
    "\n",
    "# standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# standard module for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard research-level machine learning toolkit from Meta (FKA: FaceBook)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dt\n",
    "\n",
    "# module to access data in Hieracrchical Data Format (HDF or H5 format)\n",
    "import h5py\n",
    "\n",
    "# module to plot pixelized images\n",
    "import imageio.v3 as im\n",
    "\n",
    "# module to reimport Python modules\n",
    "import importlib\n",
    "\n",
    "# module for saving/loading serialized Python objects\n",
    "import joblib\n",
    "\n",
    "# module for shell utilities\n",
    "import shutil\n",
    "\n",
    "# ML in physics module\n",
    "import mlinphysics.nn as mlp\n",
    "import mlinphysics.utils.data as dat\n",
    "import mlinphysics.utils.monitor as mon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095e404",
   "metadata": {},
   "source": [
    "Computational Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\n\\tAvailable device: {str(DEVICE):4s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd31628",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Loading the data sets from files into python}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "produced_photons_array = np.loadtxt('scan_CsI_Se82_master_produced_photon.dat')\n",
    "absorbed_photons_array = np.loadtxt('scan_CsI_Se82_master_absorbed_photon.dat')\n",
    "observed_photon_array_array = np.loadtxt('scan_CsI_Se82_master_observed_photon_array.dat')\n",
    "observed_photon_total_array = np.loadtxt('scan_CsI_Se82_master_observed_photon_total.dat')\n",
    "beam_position_array = np.loadtxt('scan_CsI_Se82_master_beam_position.dat')\n",
    "\n",
    "#Converting the beam position array to mm (values are currently in cm)\n",
    "beam_position_array = beam_position_array * 10\n",
    "\n",
    "#Normalizing each PMT of the array to the total number of photons seen by the PMTs\n",
    "normalized_PMT_array = np.zeros((1296,16))\n",
    "\n",
    "for i in range (1296):\n",
    "    for m in range (16):\n",
    "        normalized_PMT_array[i][m] = observed_photon_array_array[i][m] / observed_photon_total_array[i]\n",
    "\n",
    "# print (normalized_PMT_array.shape)\n",
    "\n",
    "print(beam_position_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea81c4",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Separating the data sets into training, validating, and testing}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a random sequence of integers from 0-1295\n",
    "randomized_indices = np.random.permutation(1296)\n",
    "\n",
    "training_indices = randomized_indices[0:648]\n",
    "validation_indices = randomized_indices[648:972]\n",
    "evaluating_model_indicies = randomized_indices[972:1296]\n",
    "\n",
    "# training, validation, and evaluation photon datasets\n",
    "training_photon_arrays = np.zeros((648,16))\n",
    "validation_photon_arrays = np.zeros((324,16))\n",
    "evaluation_photon_arrays = np.zeros((324,16))\n",
    "\n",
    "# training, validation, and evaluation position datasets\n",
    "training_position_arrays = np.zeros((648,2))\n",
    "validation_position_arrays = np.zeros((324,2))\n",
    "evaluation_position_arrays = np.zeros((324,2))\n",
    "\n",
    "for i in range (648):\n",
    "    # filling the training photon datasets\n",
    "    training_photon_arrays[i][:] = normalized_PMT_array[training_indices[i]][:]\n",
    "\n",
    "    # filling the training position datasets\n",
    "    training_position_arrays[i][:] = beam_position_array[training_indices[i], 0:2]\n",
    "    \n",
    "\n",
    "for i in range (324):\n",
    "    # filling the validation and evaluation (testing) photon datasets\n",
    "    validation_photon_arrays[i][:] = normalized_PMT_array[validation_indices[i]][:]\n",
    "    evaluation_photon_arrays[i][:] = normalized_PMT_array[evaluating_model_indicies[i]][:]\n",
    "\n",
    "    # filling the validation and evaluation (testing) position datasets\n",
    "    validation_position_arrays[i][:] = beam_position_array[validation_indices[i]][0:2]\n",
    "    evaluation_position_arrays[i][:] = beam_position_array[evaluating_model_indicies[i]][0:2]\n",
    "\n",
    "#reshaping the normalized data so that we have 432 images of 4x4 pixels\n",
    "training_photons = training_photon_arrays.reshape(648, 4, 4)\n",
    "validation_photons = validation_photon_arrays.reshape(324, 4, 4)\n",
    "evaluating_photons = evaluation_photon_arrays.reshape(324, 4, 4)\n",
    "\n",
    "print('training idices: ',training_indices[0])\n",
    "\n",
    "print('training position', training_position_arrays[0][:])\n",
    "\n",
    "print('training position shape: ', training_position_arrays.shape)\n",
    "\n",
    "print ('training photons', training_photon_arrays[0][:])\n",
    "\n",
    "###############################################\n",
    "\n",
    "# REMEMBER THAT IF YOU WANT TO COMPARE THE INDEX THAT YOU GET WITH THE DATA SET,\n",
    "# YOU MUST ADD +1 TO THE INDEX VALUE. THE POSSIBLE NUMBERS RANGE FROM 0 TO 1296-1 (1295)\n",
    "# WHILE THE DATA SET STARTS FROM 1\n",
    "\n",
    "###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eec6f3",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Preparing the datasets so that they are in the shape of (N, C, H, W)}}$\n",
    "\n",
    "$\\newline$\n",
    "N - number of images\n",
    "$\\newline$\n",
    "C - number of input channels \n",
    "$\\newline$\n",
    "H - height of our image in pixels\n",
    "$\\newline$\n",
    "W - width of our image in pixels \n",
    "$\\\\ \\text{   }$\n",
    "$\\\\$\n",
    "Also changing the data types to tensors and then the type to float in order to match that of the biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training photons\n",
    "training_photons = training_photons.reshape(648, 1, 4, 4)\n",
    "training_photons = torch.from_numpy(training_photons.astype(np.float32))\n",
    "training_position_arrays = torch.from_numpy(training_position_arrays.astype(np.float32))\n",
    "\n",
    "# Validation photons\n",
    "validation_photons = validation_photons.reshape(324, 1, 4, 4)\n",
    "validation_photons = torch.from_numpy(validation_photons.astype(np.float32))\n",
    "validation_position_arrays = torch.from_numpy(validation_position_arrays.astype(np.float32))\n",
    "\n",
    "# Evaluation photons\n",
    "evaluating_photons = evaluating_photons.reshape(324, 1, 4, 4)\n",
    "evaluating_photons = torch.from_numpy(evaluating_photons.astype(np.float32))\n",
    "evaluation_position_arrays = torch.from_numpy(evaluation_position_arrays.astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef17a9",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Configuration of the model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba225ce3",
   "metadata": {},
   "source": [
    "According to this stackexchange conversation\n",
    "\n",
    "https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
    "\n",
    "one epoch = one forward pass and one backward pass of all the training examples\n",
    "\n",
    "batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need\n",
    "\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. \n",
    "    To be clear, one pass = one forward pass + one backward pass (forward and backward passes are not counted as two different passes)\n",
    "\n",
    "For example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e94ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'impinging_position'\n",
    "\n",
    "# choose whether to create or load a configuration file\n",
    "load_existing_config = False\n",
    "\n",
    "if load_existing_config:\n",
    "    config = mlp.Config(f'{model_name}.yaml')\n",
    "else:\n",
    "    # create new config\n",
    "    config = mlp.Config(model_name)\n",
    "\n",
    "    n_images = 1296\n",
    "    batch_size = 18\n",
    "    n_iters_per_epoch = 36 # number of iterations per epoch\n",
    "    train_size = n_iters_per_epoch * batch_size\n",
    "    test_size = 324\n",
    "\n",
    "    val_size = n_images - train_size - test_size\n",
    "\n",
    "    config('batch_size', batch_size)\n",
    "    config('train_size', train_size)\n",
    "    config('test_size', test_size)\n",
    "    config('val_size', val_size)\n",
    "\n",
    "    config('monitor_step', 27) # set monitor training every n\n",
    "    config('delete', True) # delete losses file before training, if True\n",
    "    config('frac', 0.015) # save model if average loss decreases by more than frac percent\n",
    "\n",
    "    config('n_epochs', 200)\n",
    "    config('n_iters_per_epoch', n_iters_per_epoch)\n",
    "    config('n_iterations', config('n_epochs') * config('n_iters_per_epoch'))\n",
    "\n",
    "    config('n_steps', 4) # number of training steps\n",
    "    config('n_iters_per_step', config('n_iterations') // config('n_steps'))\n",
    "    \n",
    "    config('base_lr', 1.e-3) # initial learning rate\n",
    "    config('gamma', 0.8) # learning rate scale factor\n",
    "\n",
    "    print(f'\\nSave configuration to file {config.cfg_filename}\\n')\n",
    "    \n",
    "    config.save()\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db73912",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Getting our data turned into the proper data set so that it can be used with the data loaders}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de159538",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dat)\n",
    "\n",
    "train_size = config('train_size')\n",
    "val_size = config('val_size')\n",
    "test_size = config('test_size')\n",
    "\n",
    "# training dataset (this defines the empirical risk to be minimized)\n",
    "print('training data')\n",
    "train_data = dat.Dataset(training_photons, start = 0, end = train_size, targets = training_position_arrays)\n",
    "\n",
    "# a random subset of the training data to check for overtraining\n",
    "# by comparing with the empirical risk from the validation set\n",
    "\n",
    "print('training data for validation')\n",
    "train_data_val = dat.Dataset(training_photons, start = 0, end = train_size, targets = training_position_arrays, random_sample_size = val_size)\n",
    "\n",
    "# validation dataset (for monitoring training)\n",
    "print('validation data')\n",
    "val_data = dat.Dataset(validation_photons, start = 0, end = val_size, targets = validation_position_arrays)\n",
    "\n",
    "# test dataset\n",
    "print('test data')\n",
    "test_data = dat.Dataset(evaluating_photons, start = 0, end = test_size, targets = evaluation_position_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe662",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Now we include the data loaders}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train data loader')\n",
    "train_loader = dt.DataLoader(train_data, batch_size = config('batch_size'), shuffle = True)\n",
    "\n",
    "print('train data loader for validation')\n",
    "train_loader_val = dt.DataLoader(train_data_val, batch_size = len(train_data_val))\n",
    "\n",
    "print('validation data loader')\n",
    "val_loader = dt.DataLoader(val_data, batch_size = len(val_data))\n",
    "\n",
    "print('test data loader')\n",
    "test_loader = dt.DataLoader(test_data, batch_size = len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16ad27",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Now we build our model}}$\n",
    "$\\newline$\n",
    "One of the main differences between our model and that proposed in Assignment 3 is that we do not use the maxpool function, and we do not do a softmax. The first change, is due to the small size of our image 4x4. The second change is due to our outputs being spanning a continuous range from -35 to 35 mm (of course the predicted outputs may deviate between these bounds) rather than discrete categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, padding = 1, dropout = 0.08):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 2 * padding + 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, \n",
    "                               kernel_size = kernel_size, stride = 1, padding = padding, padding_mode = 'replicate')\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class BeamPositionIdentifier(mlp.Model):\n",
    "    def __init__(self, image_size = 4, channels = (1, 8, 16), padding = 1, n_outputs = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of convolutional layers\n",
    "        self.nlayers = len(channels) - 1\n",
    "\n",
    "        #building convolution layers\n",
    "        self.convs = nn.ModuleList(\n",
    "            [ConvBlock(channels[i], channels[i+1], padding)\n",
    "             for i in range(self.nlayers)])\n",
    "        \n",
    "        # computing size after convolutions\n",
    "        final_image_size = image_size\n",
    "        ninputs = channels[-1] * (final_image_size**2)\n",
    "\n",
    "        # Regression (Rather than using maxpool as we want to predict where the beam impinges upon the scintillating crystal (x,y) )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(), # Fllatens a contiguous range of dims into a tensor\n",
    "            nn.Linear(ninputs, 64), # a linear transformation from 256 (16*16 inputs) to 64 outputs\n",
    "            nn.ReLU(), # non linear function\n",
    "            nn.Linear(64, n_outputs) # linear function that transforms 64 to 2 outputs (x,y)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d260c",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Instantiate model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mlp)\n",
    "\n",
    "model = BeamPositionIdentifier().to(DEVICE)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "print('number of parameters: ', mlp.number_of_parameters(model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186ba54",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Mean Squared Error Loss function}}$\n",
    "\n",
    "$\\newline$\n",
    "Using the MSE rather than the Average cross entropy loss since we are using regression rather than classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1354e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss():\n",
    "    def __init__(self):\n",
    "        self.loss_function = nn.MSELoss()\n",
    "    def __call__(self, outputs, targets):\n",
    "        return self.loss_function(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8679161",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Instantiate training objects}}$\n",
    "\n",
    "$\\newline$\n",
    "1. optimizer\n",
    "$\\newline$\n",
    "2. scheduler\n",
    "$\\newline$\n",
    "3. objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = config('base_lr'))\n",
    "\n",
    "scheduler = mlp.get_steplr_scheduler(optimizer, config)\n",
    "\n",
    "objective = mlp.Objective(model, MeanSquaredErrorLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf8769",
   "metadata": {},
   "source": [
    "$\\underline{\\text{How we are defining accuracy}}$\n",
    "\n",
    "$\\newline$\n",
    "Since our output is continuous values, rather than the discrete categories as with assignment 3 we must redefine how we determine our model's accuracy. Let us consider the distance between predicted and the true value as our accuracy. The function \"accuracy\" takes the outputs from our model and our target values and spits out the average of the discrepancy between the two vectors. So for example if we had (0.0, 0.1) as our output and (0.0, 0.0) as our true value for one run and we only consider one run. Then the accuracy function would output (0.0) for the discrepancy in x and (0.1) for the discrepancy in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    \n",
    "    # vector_distance = targets - outputs\n",
    "    # distance = torch.sqrt(torch.sum(vector_distance**2))\n",
    "\n",
    "    abs_discrepancy = torch.abs(targets - outputs)\n",
    "\n",
    "    x_discrepancy = abs_discrepancy[:, 0].mean()\n",
    "    y_discrepancy = abs_discrepancy[:, 1].mean()\n",
    "    \n",
    "    return x_discrepancy, y_discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ca191",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Defining the Trainer}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(objective, optimizer, scheduler, train_loader, train_small_loader, val_loader, config):\n",
    "    \n",
    "    # get configuration info\n",
    "    lossfile = config('file/losses')\n",
    "    paramsfile = config('file/params')\n",
    "    step = config('monitor_step')\n",
    "    delete = config('delete')\n",
    "    frac = config('frac')\n",
    "    nepochs = config('n_epochs')\n",
    "    niters = config('n_iterations')\n",
    "\n",
    "    # instantiate objects taht saves MSE losses to a csv file for realtime monitoring\n",
    "\n",
    "    losswriter = mon.LossWriter(niters, lossfile, step = step, delete = delete, frac = frac, model = objective.model, paramsfile = paramsfile)\n",
    "\n",
    "    # instantiate learning rate step scheduler\n",
    "    lrscheduler = mlp.LRStepScheduler(optimizer, scheduler)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # training loop\n",
    "    # ---------------------------------------\n",
    "    ii = -1\n",
    "\n",
    "    for epoch in range (nepochs):\n",
    "        for x, y in train_loader:\n",
    "            ii += 1\n",
    "\n",
    "            # set mode to training so that training-specific\n",
    "            # operations such as dropout, etc., are enabled.\n",
    "            objective.model.train()\n",
    "\n",
    "            # clear all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute empirical risk\n",
    "            R = objective(x,y)\n",
    "\n",
    "            # compute gradients\n",
    "            R.backward()\n",
    "\n",
    "            # take one step downhill in the empirical risk landscape\n",
    "            optimizer.step()\n",
    "\n",
    "            # check wheter to update learning rate\n",
    "            lrscheduler.step()\n",
    "\n",
    "            # I'm alive printout\n",
    "            if (ii % step == 0) or (ii == niters -1):\n",
    "                # compute average losses on training and validation data\n",
    "                t_loss = mlp.compute_avg_loss(objective, train_small_loader)\n",
    "                v_loss = mlp.compute_avg_loss(objective, val_loader)\n",
    "                # return current learning rate\n",
    "                lr = lrscheduler.lr()\n",
    "                # update loss file\n",
    "                losswriter(ii, t_loss, v_loss, lr, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b181096",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mlp)\n",
    "importlib.reload(mon)\n",
    "\n",
    "train(objective, optimizer, scheduler, train_loader, train_loader_val, val_loader, config)\n",
    "\n",
    "monitor = mon.Monitor(config('file/losses'))\n",
    "monitor.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae339b7",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Compute accuracy on test set}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0401b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(config('file/params'))\n",
    "\n",
    "test_x, test_y = next(iter(test_loader))\n",
    "\n",
    "y_pred = model(test_x)\n",
    "\n",
    "#Since acc is a tuple with 2 tensors (accuracy of x and accuracy of 1)\n",
    "acc = accuracy(y_pred, test_y)\n",
    "\n",
    "x_accuracy = acc[0]\n",
    "x_accuracy = x_accuracy.detach().numpy()\n",
    "y_accuracy = acc[1]\n",
    "y_accuracy = y_accuracy.detach().numpy()\n",
    "\n",
    "print('Average discrepancy on x (mm) : ', x_accuracy)\n",
    "print('Average discrepancy on y (mm) : ', y_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1d1a0",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Plots of Predicted and Test data (X)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting test_y and y_pred into numpy arrays\n",
    "test_y = test_y.detach().numpy()\n",
    "y_pred = y_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6db704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('shape of test_y', test_y.shape)\n",
    "# print('test_y[:][0]', test_y[:,0])\n",
    "# print('test_y', test_y)\n",
    "\n",
    "plt.plot(y_pred[:, 0], test_y[:, 0], 'bs', label = 'test vs predicted x-positions')\n",
    "plt.plot(test_y[:, 0], test_y[:, 0], 'r--', label  = 'predicted x-positions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"predicted x values (mm)\")\n",
    "plt.ylabel(\"test x values (mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252ccc6",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Plots of Predicted and Test data (Y)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0603da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('shape of test_y', test_y.shape)\n",
    "# print('test_y[:][0]', test_y[:,1])\n",
    "# print('test_y', test_y)\n",
    "\n",
    "plt.plot(y_pred[:, 1], test_y[:, 1], 'bs', label = 'test vs predicted y-positions')\n",
    "plt.plot(test_y[:, 1], test_y[:, 1], 'r--', label  = 'predicted y-positions')\n",
    "plt.legend()\n",
    "plt.xlabel(\"predicted y values (mm)\")\n",
    "plt.ylabel(\"test y values (mm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25009bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
