{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c264416",
   "metadata": {},
   "source": [
    "Goal is to develop machine learning code that takes input photon data from an array of my hodoscope and outputs where the beam impinged on the scintillating crystal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f08fd6",
   "metadata": {},
   "source": [
    "Google Colab Installation of mlinphysics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b23e70f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running locally\n",
      "\n"
     ]
    }
   ],
   "source": [
    "COLAB_FOLDER = 'MLP' # change as needed\n",
    "GITHUB_USER  = 'hbprosper'\n",
    "GITHUB_REPO  = 'mlinphysics'\n",
    "GITHUB_FOLDERS = ['mlinphysics']\n",
    "#------------------------------------------------------\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    print('\\nGoogle Drive mounted\\n')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    print('\\nRunning locally\\n')\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    MYDRIVE     = '/content/gdrive/MyDrive'\n",
    "    GITHUB_BASE = 'https://raw.githubusercontent.com'\n",
    "    MAIN        = 'refs/heads/main'\n",
    "    GITHUB_PATH = f'{MYDRIVE}/{COLAB_FOLDER}'\n",
    "    #------------------------------------------------------\n",
    "    %cd {GITHUB_PATH}\n",
    "    %rm -f {GITHUB_PATH}/clone2colab.ipynb\n",
    "    !wget -q {GITHUB_BASE}/{GITHUB_USER}/{GITHUB_REPO}/{MAIN}/clone2colab.ipynb\n",
    "    %run {GITHUB_PATH}/clone2colab.ipynb\n",
    "\n",
    "    %pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb438b52",
   "metadata": {},
   "source": [
    "Importing the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c8c306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard system modules\n",
    "import os, sys\n",
    "\n",
    "# standard module for tabular data\n",
    "import pandas as pd\n",
    "\n",
    "# standard module for array manipulation\n",
    "import numpy as np\n",
    "\n",
    "# standard module for high-quality plots\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# standard research-level machine learning toolkit from Meta (FKA: FaceBook)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dt\n",
    "\n",
    "# module to access data in Hieracrchical Data Format (HDF or H5 format)\n",
    "import h5py\n",
    "\n",
    "# module to plot pixelized images\n",
    "import imageio.v3 as im\n",
    "\n",
    "# module to reimport Python modules\n",
    "import importlib\n",
    "\n",
    "# module for saving/loading serialized Python objects\n",
    "import joblib\n",
    "\n",
    "# module for shell utilities\n",
    "import shutil\n",
    "\n",
    "# ML in physics module\n",
    "import mlinphysics.nn as mlp\n",
    "import mlinphysics.utils.data as dat\n",
    "import mlinphysics.utils.monitor as mon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095e404",
   "metadata": {},
   "source": [
    "Computational Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5950787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tAvailable device: cpu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'\\n\\tAvailable device: {str(DEVICE):4s}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd31628",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Loading the data sets}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8021704f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.5 -3.5 -8.   0.   0.   1. ]\n",
      " [-3.5 -3.3 -8.   0.   0.   1. ]\n",
      " [-3.5 -3.1 -8.   0.   0.   1. ]\n",
      " ...\n",
      " [ 3.5  3.1 -8.   0.   0.   1. ]\n",
      " [ 3.5  3.3 -8.   0.   0.   1. ]\n",
      " [ 3.5  3.5 -8.   0.   0.   1. ]]\n"
     ]
    }
   ],
   "source": [
    "produced_photons_array = np.loadtxt('scan_CsI_Se82_master_produced_photon.dat')\n",
    "absorbed_photons_array = np.loadtxt('scan_CsI_Se82_master_absorbed_photon.dat')\n",
    "observed_photon_array_array = np.loadtxt('scan_CsI_Se82_master_observed_photon_array.dat')\n",
    "observed_photon_total_array = np.loadtxt('scan_CsI_Se82_master_observed_photon_total.dat')\n",
    "beam_position_array = np.loadtxt('scan_CsI_Se82_master_beam_position.dat')\n",
    "\n",
    "#Normalizing each PMT of the array to the total number of photons seen by the PMTs\n",
    "normalized_PMT_array = np.zeros((1296,16))\n",
    "\n",
    "for i in range (1296):\n",
    "    for m in range (16):\n",
    "        normalized_PMT_array[i][m] = observed_photon_array_array[i][m] / observed_photon_total_array[i]\n",
    "\n",
    "# print (normalized_PMT_array.shape)\n",
    "\n",
    "print(beam_position_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea81c4",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Separating the data sets into training, validating, and testing}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af24113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training idices:  743\n",
      "training position [0.5 1.1]\n",
      "training photons [0.06548553 0.06799716 0.06473239 0.06271287 0.06586108 0.0681805\n",
      " 0.06479305 0.06316339 0.06308296 0.06423892 0.06205037 0.05987749\n",
      " 0.05740199 0.05946105 0.05641711 0.05454413]\n"
     ]
    }
   ],
   "source": [
    "# generates a random sequence of integers from 0-1295\n",
    "randomized_indices = np.random.permutation(1296)\n",
    "\n",
    "training_indices = randomized_indices[0:648]\n",
    "validation_indices = randomized_indices[648:972]\n",
    "evaluating_model_indicies = randomized_indices[972:1296]\n",
    "\n",
    "# training, validation, and evaluation photon datasets\n",
    "training_photon_arrays = np.zeros((648,16))\n",
    "validation_photon_arrays = np.zeros((324,16))\n",
    "evaluation_photon_arrays = np.zeros((324,16))\n",
    "\n",
    "# training, validation, and evaluation position datasets\n",
    "training_position_arrays = np.zeros((648,2))\n",
    "validation_position_arrays = np.zeros((324,2))\n",
    "evaluation_position_arrays = np.zeros((324,2))\n",
    "\n",
    "for i in range (648):\n",
    "    # filling the training photon datasets\n",
    "    training_photon_arrays[i][:] = normalized_PMT_array[training_indices[i]][:]\n",
    "\n",
    "    # filling the training position datasets\n",
    "    training_position_arrays[i][:] = beam_position_array[training_indices[i], 0:2]\n",
    "    \n",
    "\n",
    "for i in range (324):\n",
    "    # filling the validation and evaluation (testing) photon datasets\n",
    "    validation_photon_arrays[i][:] = normalized_PMT_array[validation_indices[i]][:]\n",
    "    evaluation_photon_arrays[i][:] = normalized_PMT_array[evaluating_model_indicies[i]][:]\n",
    "\n",
    "    # filling the validation and evaluation (testing) position datasets\n",
    "    validation_position_arrays[i][:] = beam_position_array[validation_indices[i]][0:2]\n",
    "    evaluation_position_arrays[i][:] = beam_position_array[evaluating_model_indicies[i]][0:2]\n",
    "\n",
    "#reshaping the normalized data so that we have 432 images of 4x4 pixels\n",
    "training_photons = training_photon_arrays.reshape(648, 4, 4)\n",
    "validation_photons = validation_photon_arrays.reshape(324, 4, 4)\n",
    "evaluating_photons = evaluation_photon_arrays.reshape(324, 4, 4)\n",
    "\n",
    "print('training idices: ',training_indices[0])\n",
    "\n",
    "print('training position', training_position_arrays[0][:])\n",
    "\n",
    "print ('training photons', training_photon_arrays[0][:])\n",
    "###############################################\n",
    "\n",
    "# REMEMBER THAT IF YOU WANT TO COMPARE THE INDEX THAT YOU GET WITH THE DATA SET,\n",
    "# YOU MUST ADD +1 TO THE INDEX VALUE. THE POSSIBLE NUMBERS RANGE FROM 0 TO 1296-1 (1295)\n",
    "# WHILE THE DATA SET STARTS FROM 1\n",
    "\n",
    "###############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eec6f3",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Preparing the datasets so that they are in the shape of (N, C, H, W)}}$\n",
    "\n",
    "$\\newline$\n",
    "N - number of images\n",
    "$\\newline$\n",
    "C - number of input channels \n",
    "$\\newline$\n",
    "H - height of our image in pixels\n",
    "$\\newline$\n",
    "W - width of our image in pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3982c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_photons = training_photons.reshape(648, 1, 4, 4)\n",
    "validation_photons = validation_photons.reshape(324, 1, 4, 4)\n",
    "evaluating_photons = evaluating_photons.reshape(324, 1, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef17a9",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Configuration of the model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba225ce3",
   "metadata": {},
   "source": [
    "According to this stackexchange conversation\n",
    "\n",
    "https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks\n",
    "\n",
    "one epoch = one forward pass and one backward pass of all the training examples\n",
    "\n",
    "batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need\n",
    "\n",
    "number of iterations = number of passes, each pass using [batch size] number of examples. \n",
    "    To be clear, one pass = one forward pass + one backward pass (forward and backward passes are not counted as two different passes)\n",
    "\n",
    "For example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e94ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save configuration to file runs/2025-11-20_1431/impinging_position_config.yaml\n",
      "\n",
      "name: impinging_position\n",
      "file:\n",
      "  losses: runs/2025-11-20_1431/impinging_position_losses.csv\n",
      "  params: runs/2025-11-20_1431/impinging_position_params.pth\n",
      "  init_params: runs/2025-11-20_1431/impinging_position_init_params.pth\n",
      "  plots: runs/2025-11-20_1431/impinging_position_plots.png\n",
      "batch_size: 18\n",
      "train_size: 648\n",
      "test_size: 324\n",
      "val_size: 324\n",
      "monitor_step: 27\n",
      "delete: true\n",
      "frac: 0.015\n",
      "n_epochs: 200\n",
      "n_iters_per_epoch: 36\n",
      "n_iterations: 7200\n",
      "n_steps: 4\n",
      "n_iters_per_step: 1800\n",
      "base_lr: 0.001\n",
      "gamma: 0.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'impinging_position'\n",
    "\n",
    "# choose whether to create or load a configuration file\n",
    "load_existing_config = False\n",
    "\n",
    "if load_existing_config:\n",
    "    config = mlp.Config(f'{model_name}.yaml')\n",
    "else:\n",
    "    # create new config\n",
    "    config = mlp.Config(model_name)\n",
    "\n",
    "    n_images = 1296\n",
    "    batch_size = 18\n",
    "    n_iters_per_epoch = 36 # number of iterations per epoch\n",
    "    train_size = n_iters_per_epoch * batch_size\n",
    "    test_size = 324\n",
    "\n",
    "    val_size = n_images - train_size - test_size\n",
    "\n",
    "    config('batch_size', batch_size)\n",
    "    config('train_size', train_size)\n",
    "    config('test_size', test_size)\n",
    "    config('val_size', val_size)\n",
    "\n",
    "    config('monitor_step', 27) # set monitor training every n\n",
    "    config('delete', True) # delete losses file before training, if True\n",
    "    config('frac', 0.015) # save model if average loss decreases by more than frac percent\n",
    "\n",
    "    config('n_epochs', 200)\n",
    "    config('n_iters_per_epoch', n_iters_per_epoch)\n",
    "    config('n_iterations', config('n_epochs') * config('n_iters_per_epoch'))\n",
    "\n",
    "    config('n_steps', 4) # number of training steps\n",
    "    config('n_iters_per_step', config('n_iterations') // config('n_steps'))\n",
    "    \n",
    "    config('base_lr', 1.e-3) # initial learning rate\n",
    "    config('gamma', 0.8) # learning rate scale factor\n",
    "\n",
    "    print(f'\\nSave configuration to file {config.cfg_filename}\\n')\n",
    "    \n",
    "    config.save()\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db73912",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Getting our data turned into the proper data set so that it can be used with the data loaders}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de159538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "Dataset\n",
      "  shape of x: torch.Size([648, 1, 4, 4])\n",
      "  shape of y: torch.Size([648, 2])\n",
      "\n",
      "training data for validation\n",
      "Dataset\n",
      "  shape of x: torch.Size([324, 1, 4, 4])\n",
      "  shape of y: torch.Size([324, 2])\n",
      "\n",
      "validation data\n",
      "Dataset\n",
      "  shape of x: torch.Size([324, 1, 4, 4])\n",
      "  shape of y: torch.Size([324, 2])\n",
      "\n",
      "test data\n",
      "Dataset\n",
      "  shape of x: torch.Size([324, 1, 4, 4])\n",
      "  shape of y: torch.Size([324, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(dat)\n",
    "\n",
    "train_size = config('train_size')\n",
    "val_size = config('val_size')\n",
    "test_size = config('test_size')\n",
    "\n",
    "# training dataset (this defines the empirical risk to be minimized)\n",
    "print('training data')\n",
    "train_data = dat.Dataset(training_photons, start = 0, end = train_size, targets = training_position_arrays)\n",
    "\n",
    "# a random subset of the training data to check for overtraining\n",
    "# by comparing with the empirical risk from the validation set\n",
    "\n",
    "print('training data for validation')\n",
    "train_data_val = dat.Dataset(training_photons, start = 0, end = train_size, targets = training_position_arrays, random_sample_size = val_size)\n",
    "\n",
    "# validation dataset (for monitoring training)\n",
    "print('validation data')\n",
    "val_data = dat.Dataset(validation_photons, start = 0, end = val_size, targets = validation_position_arrays)\n",
    "\n",
    "# test dataset\n",
    "print('test data')\n",
    "test_data = dat.Dataset(evaluating_photons, start = 0, end = test_size, targets = evaluation_position_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861fe662",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Now we include the data loaders}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c798bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loader\n",
      "train data loader for validation\n",
      "validation data loader\n",
      "test data loader\n"
     ]
    }
   ],
   "source": [
    "print('train data loader')\n",
    "train_laoder = dt.DataLoader(train_data, batch_size = config('batch_size'), shuffle = True)\n",
    "\n",
    "print('train data loader for validation')\n",
    "train_loader_val = dt.DataLoader(train_data_val, batch_size = len(train_data_val))\n",
    "\n",
    "print('validation data loader')\n",
    "val_loader = dt.DataLoader(val_data, batch_size = len(val_data))\n",
    "\n",
    "print('test data loader')\n",
    "test_loader = dt.DataLoader(test_data, batch_size = len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16ad27",
   "metadata": {},
   "source": [
    "$\\underline{\\textbf{Now we build our model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b2c8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, padding = 1, dropout = 0.08):\n",
    "        super().__init__()\n",
    "\n",
    "        kernel_size = 2 * padding + 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, \n",
    "                               kernel_size = kernel_size, stride = 1, padding = padding, padding_mode = 'replicate')\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class BeamPositionIdentifier(mlp.Model):\n",
    "    def __init__(self, image_size = 4, channels = (1, 8, 16), padding = 1, n_outputs = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of convolutional layers\n",
    "        self.nlayers = len(channels) - 1\n",
    "\n",
    "        #building convolution layers\n",
    "        self.convs = nn.ModuleList(\n",
    "            [ConvBlock(channels[i], channels[i+1], padding)\n",
    "             for i in range(self.nlayers)])\n",
    "        \n",
    "        # computing size after convolutions\n",
    "        final_image_size = image_size\n",
    "        ninputs = channels[-1] * (final_image_size**2)\n",
    "\n",
    "        # Regression (Rather than using maxpool as we want to predict where the beam impinges upon the scintillating crystal (x,y) )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(ninputs, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, n_outputs)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d260c",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Instantiate model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5c9e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeamPositionIdentifier(\n",
      "  (convs): ModuleList(\n",
      "    (0): ConvBlock(\n",
      "      (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.08, inplace=False)\n",
      "    )\n",
      "    (1): ConvBlock(\n",
      "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=replicate)\n",
      "      (relu): ReLU()\n",
      "      (dropout): Dropout(p=0.08, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "number of parameters:  17826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(mlp)\n",
    "\n",
    "model = BeamPositionIdentifier().to(DEVICE)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "print('number of parameters: ', mlp.number_of_parameters(model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186ba54",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Mean Squared Error Loss function}}$\n",
    "\n",
    "$\\newline$\n",
    "Using the MSE rather than the Average cross entropy loss since we are using regression rather than classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1354e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredErrorLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, outputs, targets):\n",
    "\n",
    "        loss = nn.MSELoss()\n",
    "\n",
    "        result = loss(outputs, targets)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8679161",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Instantiate training objects}}$\n",
    "\n",
    "$\\newline$\n",
    "1. optimizer\n",
    "$\\newline$\n",
    "2. scheduler\n",
    "$\\newline$\n",
    "3. objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = config('base_lr'))\n",
    "\n",
    "scheduler = mlp.get_stepler_scheduler(optimizer, config)\n",
    "\n",
    "objective = mlp.Objective(model, MeanSquaredErrorLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c7aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    number_correct = float (np.mean(outputs == targets.data.cpu().numpy()))\n",
    "    return number_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ca191",
   "metadata": {},
   "source": [
    "$\\underline{\\text{Defining the Trainer}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(objective, optimizer, scheduler, train_loader, train_small_loader, val_loader, config):\n",
    "    \n",
    "    # get configuration info\n",
    "    lossfile = config('file/losses')\n",
    "    paramsfile = config('file/params')\n",
    "    step = config('monitor_step')\n",
    "    delete = config('delete')\n",
    "    frac = config('frac')\n",
    "    nepochs = config('n_epochs')\n",
    "    niters = config('n_iterations')\n",
    "\n",
    "    # instantiate objects taht saves MSE losses to a csv file for realtime monitoring\n",
    "\n",
    "    losswriter = mon.LossWriter(niters, lossfile, step = step, frac = frac, model = objective.model, paramsfile = paramsfile)\n",
    "\n",
    "    # instantiate learning rate step scheduler\n",
    "    lrscheduler = mlp.LRStepScheduler(optimizer, scheduler)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # training loop\n",
    "    # ---------------------------------------\n",
    "    ii = -1\n",
    "\n",
    "    for epoch in range (nepochs):\n",
    "        for x, y in train_loader:\n",
    "            ii += 1\n",
    "\n",
    "            # set mode to training so that training-specific\n",
    "            # operations such as dropout, etc., are enabled.\n",
    "            objective.train()\n",
    "\n",
    "            # clear all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # compute empirical risk\n",
    "            R = objective(x,y)\n",
    "\n",
    "            # compute gradients\n",
    "            R.backward()\n",
    "\n",
    "            # take one step downhill in the empirical risk landscape\n",
    "            optimizer.step()\n",
    "\n",
    "            # check wheter to update learning rate\n",
    "            lrscheduler.step()\n",
    "\n",
    "            # I'm alive printout\n",
    "            if (ii % step == 0) or (ii == niters -1):\n",
    "                # compute average losses on training and validation data\n",
    "                t_loss = mlp.compute_avg_loss(objective, train_small_loader)\n",
    "                v_loss = mlp.compute_avg_loss(objective, val_loader)\n",
    "                # return current learning rate\n",
    "                lr = lrscheduler.lr()\n",
    "                # update loss file\n",
    "                losswriter(ii, t_loss, v_loss, lr, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b181096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
